{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Algorithm\n",
    "The program uses PPO, an actor critic method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization\n",
    "Proximal Policy Optimization, or PPO, is one of the most widely used Reinforcement Learning algorithms. It is both simple, and highly effective.\n",
    "\n",
    "PPO uses two neural networks, one is called the Actor, given a state, it returns the probability that the agent will take each action. The second neural network is called the Critic which takes a state, and returns the expected reward for said state. By combining these two networks, the Critic can help tell the Actor whether it took a good or bad action, allowing the program to learn faster.\n",
    "\n",
    "# Trajectories\n",
    "In order to train, PPO must first collect a series of trajectories. The complete set of state, actions, and rewards for an episode played by the agent. In general, the more trajectories collected before the agent is updated, the faster the agent will learn\n",
    "\n",
    "# Clipped Loss Function\n",
    "One problem with almost all Reinforcement Learning algorithms is that they are highly unstable. When a large update is made to the policy, there is a chance for the change to be too big and greatly harm the performance. To fix this, PPO uses the clipped loss function shown below. \n",
    "\n",
    "$$min((\\frac{\\pi(a|s)}{\\pi_{old}(a|s)})A(s,a),clip(\\frac{\\pi(a|s)}{\\pi_{old}(a|s)},1-\\varepsilon,1+\\varepsilon)A(s,a))$$\n",
    "\n",
    "PPO introduces a clipping term to the loss function, taking away the incentive to change the loss function too much. This way, if the ratio of the current action probability over the old action probability is greater than 1 + epsilon, or less than 1 - epsilon, the loss function will not encourage the policy to increase or decrease respectively.\n",
    "\n",
    "In continuous action spaces, such as this environment, the probability density function is used rather than just calculating the probability. The reason for this is because the probability of a specific value being picked from any range is 0 due to the infinate amount of choices. \n",
    "\n",
    "# Generalized advantage estimation\n",
    "In the loss function showed above, the Advantage function was shown, in the version of PPO I implemented, I used Generalized Advantage Estimation in order to calculate the advantage. \n",
    "\n",
    "Generalized advantage estimation is method for estimating an advantage function that combines the discounted rewards, which usually vary greatly but are generally accurate, with the outputs from the Critic Network, which vary much less, but are less accurate than the discounted rewards. It has been shown that by finding a balance between accuracy, and variance, it is possible for a reinforcement learning program to train much faster.\n",
    "\n",
    "# Continuous Action Space\n",
    "Unlike the previous project, this environment contains a continuous action space. Each action can be any value between -1 and 1. Due to this, the Actor net had to output the mean of a multivariate distribution which was used to come up with values for the action, and to calculate the probability density for the loss function.  \n",
    "\n",
    "# Network Architecture\n",
    "The Policy Network has 2 hidden layers, both with a tanh activation and 64 neurons. Its input is size 33 vector, and its output is a size 4 vector. The output layer also has a tanh activation\n",
    "\n",
    "The Critic Network is the same, but it only has one output. The Critic Network's output layer also has a tanh activation. This was done because there was a low discount value, meaning the discounted rewards were usually in between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "Actor Learning Rate: 0.001, getting multiplied by 0.996 every episode\n",
    "\n",
    "Critic Learning Rate: \n",
    "\n",
    "Update Frequency: Every episode\n",
    "\n",
    "Actor mini-batch size: 150\n",
    "\n",
    "Actor batch size: 100\n",
    "\n",
    "Critic mini-batch size: 200\n",
    "\n",
    "Critic Batch size: 200, then 50 after first 10 games\n",
    "\n",
    "Epsilon: 0.2\n",
    "\n",
    "Discount: 0.96\n",
    "\n",
    "Residual Discount: 0.96(Used in Generalized advantage estimation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "The agent took 295 episodes to solve the environment(the average reward between 195 and 295 is 30.06). As shown in the graph below, \n",
    "the reward took a while to increase, but then continued to rise quickly. As it approached the maximum possible score, the rate at which its reward increased dropped drastically. \n",
    "The agent likely would not continue to improve its reward much with continued training time due to the fact that since the episode ends after a set time, it is impossible to \n",
    "get a score past the limit of around 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/learning_curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for Future Work\n",
    "It would be interesting to see what the effect of using PPO-penalty, which uses KL divergence rather than the clipping that was used in this implementation. The hyperparameters could also be experimented with to see wheter they could help speed up training. \n",
    "\n",
    "Other algorithms could be tried as well, such as TRPO and DDPG to compare both the sample efficiency, and the runtime for each different algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
